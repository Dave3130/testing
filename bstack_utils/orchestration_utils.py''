import os
import tempfile
import math
from bstack_utils import logger_utils
from bstack_utils.constants import DEFAULT_LOG_LEVEL, TEST_ORDERING_SUPPORTED_FRAMEWORKS
from bstack_utils.helper import get_git_metadata_for_ai_selection, get_host_info
from bstack_utils.request_utils import RequestUtils
import json
import re
import sys
RETRY_TESTS_ON_FAILURE = "retryTestsOnFailure"
ABORT_BUILD_ON_FAILURE = "abortBuildOnFailure"
RUN_PREVIOUSLY_FAILED_FIRST = "runPreviouslyFailedFirst"
RERUN_PREVIOUSLY_FAILED = "rerunPreviouslyFailed"
SKIP_FLAKY_AND_FAILED = "skipFlakyandFailed"
RUN_SMART_SELECTION = "runSmartSelection"

ALLOWED_ORCHESTRATION_KEYS = {
    RETRY_TESTS_ON_FAILURE,
    ABORT_BUILD_ON_FAILURE,
    RUN_PREVIOUSLY_FAILED_FIRST,
    RERUN_PREVIOUSLY_FAILED,
    SKIP_FLAKY_AND_FAILED,
    RUN_SMART_SELECTION
}

ABORT_BUILD_SUPPORTED_FRAMEWORKS = {'pytest'}
logger = logger_utils.get_logger(__name__, DEFAULT_LOG_LEVEL)

class TestOrdering:
    def __init__(self):
        self.enabled = False
        self.name = None

    def enable(self, name):
        self.enabled = True
        self.name = name

    def disable(self):
        self.enabled = False
        self.name = None

    def get_enabled(self):
        return self.enabled

    def get_name(self):
        return self.name

class OrchestrationUtils:
    _instance = None
    
    def __init__(self, config):
        self.run_previously_failed_first = False
        self.rerun_previously_failed = False
        self.skip_flaky_and_failed = False
        self.run_smart_selection = False
        self.smart_selection_mode = None
        self.test_ordering = TestOrdering()
        self.smart_selection_source = None  # Store source paths if provided
        opts = config.get('testOrchestrationOptions', {})
        self.smartSelectionFeatureBranchesENV = config.get('smartSelectionFeatureBranchesENV', "")
        self.smartSelectionFeatureBranchesCLI = config.get('smartSelectionFeatureBranchesCLI', "")
        run_smart_selection_opts = opts.get(RUN_SMART_SELECTION, {})
        # If source key is not present in yml, it should be treated as None, if key is present but value is null, it should be treated as empty list
        source_value = None
        if 'source' in run_smart_selection_opts:
            source_value = run_smart_selection_opts['source']
            if source_value is None:
                source_value = []
        
        self.__set_run_smart_selection(
            run_smart_selection_opts.get('enabled', False),
            run_smart_selection_opts.get('mode', 'relevantFirst'),
            source_value
        )

        self.__set_run_previously_failed_first(opts.get(RUN_PREVIOUSLY_FAILED_FIRST, False))
        self.__set_rerun_previously_failed(opts.get(RERUN_PREVIOUSLY_FAILED, False))
        self.__set_skip_flaky_and_failed(opts.get(SKIP_FLAKY_AND_FAILED, False))

    @classmethod
    def get_instance(cls, config=None):
        if cls._instance is None and config is not None:
            cls._instance = OrchestrationUtils(config)
        return cls._instance
    
    @staticmethod
    def is_retry_enabled(config: dict) -> bool:
        retry_options = config.get('testOrchestrationOptions', {}).get(RETRY_TESTS_ON_FAILURE, {})
        return retry_options.get('enabled', False)

    @staticmethod
    def get_retry_count(config: dict) -> int:
        retry_options = config.get('testOrchestrationOptions', {}).get(RETRY_TESTS_ON_FAILURE, {})
        retries = 0
        if OrchestrationUtils.is_retry_enabled(config):
            retries = retry_options.get('maxRetries', 1)
        return retries

    @staticmethod
    def get_orchestration_data(config: dict) -> dict:
        orchestration_data = config.get('testOrchestrationOptions', {})
        return {
            key: value for key, value in orchestration_data.items() if key in ALLOWED_ORCHESTRATION_KEYS
        }

    @staticmethod
    def check_abort_build_file_exists():
        """
        Check if the abort build file exists.
        """
        return os.path.exists(os.path.join(tempfile.gettempdir(), "abort_build_{}".format(os.getenv("BROWSERSTACK_TESTHUB_UUID"))))

    @staticmethod
    def write_failure_to_file(test_name: str):
        """
        Check if the abort build file exists.
        """

        failed_tests_file = os.path.join(tempfile.gettempdir(), "failed_tests_{}.txt".format(os.getenv("BROWSERSTACK_TESTHUB_UUID")))
        with open(failed_tests_file, 'a') as file:
            file.write("{}\n".format(test_name))

    @staticmethod
    def is_abort_build_on_failure_supported(framework: str) -> bool:
       return framework.lower() in ABORT_BUILD_SUPPORTED_FRAMEWORKS

    @staticmethod
    def is_abort_build_on_failure_enabled(config: dict) -> bool:
        abort_build_on_failure_options = config.get('testOrchestrationOptions', {}).get(ABORT_BUILD_ON_FAILURE, {})
        return abort_build_on_failure_options.get('enabled', False)

    @staticmethod
    def get_failure_threshold(config: dict, total_tests: int = 0) -> int:
        """
        Get the failure threshold, which can be an absolute number or a percentage.
        Args:
            config (dict): The configuration dictionary.
            total_tests (int): The total number of tests (required for percentage-based thresholds).
        Returns:
            int: The failure threshold.
        """
        abort_build_on_failure_options = config.get('testOrchestrationOptions', {}).get('abortBuildOnFailure', {})
        max_failures = 0
        max_failures_value = 0

        if OrchestrationUtils.is_abort_build_on_failure_enabled(config):
            max_failures_value = abort_build_on_failure_options.get('maxFailures', 5)

            if isinstance(max_failures_value, str) and max_failures_value.endswith('%'):
                # Handle percentage-based threshold
                try:
                    percentage = int(max_failures_value.strip('%'))
                    if total_tests > 0:
                        max_failures = math.ceil((percentage * total_tests) / 100)
                    else:
                        raise ValueError("Total tests must be provided for percentage-based thresholds.")
                except ValueError as e:
                    raise ValueError("Invalid percentage value for maxFailures: {}".format(max_failures_value)) from e
            else:
                # Handle absolute number threshold
                max_failures = int(max_failures_value)

        logger.info("Max failures threshold set to: {} (from config: {})".format(max_failures, max_failures_value))
        return max_failures

    def get_run_smart_selection(self):
        return self.run_smart_selection

    def get_smart_selection_mode(self):
        return self.smart_selection_mode

    def get_smart_selection_source(self):
        return self.smart_selection_source

    def __set_run_smart_selection(self, enabled, mode, source=None):
        try:
            self.run_smart_selection = bool(enabled)
            # Mode validation
            if mode not in ['relevantFirst', 'relevantOnly']:
                logger.warning("Invalid smart selection mode '{}' provided. Defaulting to 'relevantFirst'.".format(mode))
                mode = 'relevantFirst'

            self.smart_selection_mode = mode

            # Normalize source to always be a list of paths
            if source is None:
                self.smart_selection_source = None
            elif isinstance(source, list):
                self.smart_selection_source = source
            elif isinstance(source, str) and source.endswith('.json'):
                self.smart_selection_source = self._load_source_from_json(source)
            self.__set_test_ordering()
        except Exception as e:
            logger.error("Failed to set smart selection configuration - enabled: {}, mode: {}, source: {}. Error: {}".format(enabled, mode, source, e))

    def get_run_previously_failed_first(self):
        return self.run_previously_failed_first

    def __set_run_previously_failed_first(self, value):
        self.run_previously_failed_first = bool(value)
        self.__set_test_ordering()

    def get_rerun_previously_failed(self):
        return self.rerun_previously_failed

    def __set_rerun_previously_failed(self, value):
        self.rerun_previously_failed = bool(value)
        self.__set_test_ordering()

    def get_skip_flaky_and_failed(self):
        return self.skip_flaky_and_failed

    def __set_skip_flaky_and_failed(self, value):
        self.skip_flaky_and_failed = bool(value)
        self.__set_test_ordering()

    def __set_test_ordering(self):
        if self.run_smart_selection:  # Highest priority
            self.run_previously_failed_first = False
            self.rerun_previously_failed = False
            self.skip_flaky_and_failed = False
            self.test_ordering.enable(RUN_SMART_SELECTION)
        elif self.run_previously_failed_first:
            self.rerun_previously_failed = False
            self.skip_flaky_and_failed = False
            self.run_smart_selection = False
            self.test_ordering.enable(RUN_PREVIOUSLY_FAILED_FIRST)
        elif self.rerun_previously_failed:
            self.run_previously_failed_first = False
            self.skip_flaky_and_failed = False
            self.run_smart_selection = False
            self.test_ordering.enable(RERUN_PREVIOUSLY_FAILED)
        elif self.skip_flaky_and_failed:
            self.run_previously_failed_first = False
            self.rerun_previously_failed = False
            self.run_smart_selection = False
            self.test_ordering.enable(SKIP_FLAKY_AND_FAILED)
        else:
            self.test_ordering.disable()

    def test_ordering_enabled(self):
        return self.test_ordering.get_enabled()

    def get_test_ordering_name(self):
        if self.test_ordering.get_enabled():
            return self.test_ordering.get_name()
        return None

    def _load_source_from_json(self, source_file_path):
        """
        Parse JSON source configuration file and format it for smart selection.
        
        Args:
            source_file_path (str): Path to the JSON configuration file
            
        Returns:
            list: Formatted list of repository configurations
        """
        if not os.path.isfile(source_file_path):
            logger.error("Source file '{}' does not exist.".format(source_file_path))
            return []
        data = None
        try:
            with open(source_file_path, "r") as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            logger.error("Error parsing JSON from source file '{}': {}".format(source_file_path, e))
            return []
        
        # Cache feature branch mappings from env and CLI args to avoid repeated parsing
        _feature_branch_env_map = None
        _feature_branch_cli_map = None

        def _load_feature_branch_maps():
            env_map = {}
            cli_map = {}
            # Check if env_var and cli_var can be json also
            try:
                if self.smartSelectionFeatureBranchesENV.startswith('{') and self.smartSelectionFeatureBranchesENV.endswith('}'):
                    env_map = json.loads(self.smartSelectionFeatureBranchesENV)
                else:
                    env_map = dict(item.split(':') for item in self.smartSelectionFeatureBranchesENV.split(',') if ':' in item) if self.smartSelectionFeatureBranchesENV else {}

                if self.smartSelectionFeatureBranchesCLI.startswith('{') and self.smartSelectionFeatureBranchesCLI.endswith('}'):
                    cli_map = json.loads(self.smartSelectionFeatureBranchesCLI)
                else:
                    cli_map = dict(item.split(':') for item in self.smartSelectionFeatureBranchesCLI.split(',') if ':' in item) if self.smartSelectionFeatureBranchesCLI else {}
            except json.JSONDecodeError as e:
                logger.error("Error parsing feature branch mappings: {}".format(e))
            logger.debug("Feature branch mappings from env: {}, CLI: {}".format(env_map, cli_map))
            return env_map, cli_map

        if _feature_branch_env_map is None or _feature_branch_cli_map is None:
            _feature_branch_env_map, _feature_branch_cli_map = _load_feature_branch_maps()

        def get_feature_branch(name, repo_info):
            # 1. Check in CLI arguments map
            if name in _feature_branch_cli_map:
                return _feature_branch_cli_map[name]
            # 2. Check in environment variable map
            if name in _feature_branch_env_map:
                return _feature_branch_env_map[name]
            # 3. Check in repo_info
            if repo_info.get('featureBranch'):
                return repo_info['featureBranch']   
            return None
        
        if isinstance(data, dict):
            formatted_data = []
            name_pattern = re.compile(r'^[A-Z0-9_]+$')
            for name, repo_info in data.items():
                if not isinstance(repo_info, dict):
                    continue

                if not repo_info.get('url'):
                    logger.warning("Repository URL is missing for source '{}': {}".format(name, repo_info))
                    continue
                # Validate name
                if not name_pattern.match(name):
                    logger.warning("Invalid source identifier format for '{}': {}".format(name, repo_info))
                    continue
                # Validate length
                if len(name) > 30 or len(name) < 1:
                    logger.warning("Source identifier '{}' must have a length between 1 and 30 characters.".format(name))
                    continue
                repo_info = repo_info.copy()
                repo_info['name'] = name
                repo_info['featureBranch'] = get_feature_branch(name, repo_info)

                if not repo_info.get('featureBranch'):
                    logger.warning("Feature branch not specified for source '{}': {}".format(name, repo_info))
                    continue
                
                if repo_info.get('baseBranch') and repo_info['baseBranch'] == repo_info['featureBranch']:
                    logger.warning("Feature branch and base branch cannot be the same for source '{}': {}".format(name, repo_info))
                    continue

                formatted_data.append(repo_info)
            return formatted_data
        
        return data

    def get_test_orchestration_metadata(self):
        data = {
            'run_smart_selection': {
                'enabled': self.get_run_smart_selection(),
                'mode': self.get_smart_selection_mode(),
                'source': self.get_smart_selection_source()
            }
        }
        return data

    def get_build_start_data(self, config):
        test_orchestration_data = {}

        test_orchestration_data['run_smart_selection'] = {
            'enabled': self.get_run_smart_selection(),
            'mode': self.get_smart_selection_mode()
            # Not sending "source" to TH builds
        }
     
        test_orchestration_data['rerun_previously_failed'] = {
            'enabled': self.get_rerun_previously_failed()
        }

        test_orchestration_data['run_previously_failed_first'] = {
            'enabled': self.get_run_previously_failed_first()
        }

        test_orchestration_data['skip_failing_and_flaky'] = {
            'enabled': self.get_skip_flaky_and_failed()
        }

        if self.is_retry_enabled(config):
            test_orchestration_data['retry_tests_on_failure'] = {
                'enabled': True,
                'max_retries': self.get_retry_count(config)
            }

        if self.is_abort_build_on_failure_enabled(config):
            test_orchestration_data['abort_build_on_failure'] = {
                'enabled': True,
                'max_failures': self.get_failure_threshold(config)
            }

        return test_orchestration_data

    def collect_build_data(self, config):
        """
        Collects build data by making a call to the collect-build-data endpoint.

        Args:
            build_uuid (str): The UUID of the build to collect data for.

        Returns:
            dict: Response from the collect-build-data endpoint, or None if failed.
        """
        # Return early if smart selection is not enabled or applicable
        if not (config.get('framework', None) in TEST_ORDERING_SUPPORTED_FRAMEWORKS and self.get_run_smart_selection()):
            return None
        
        build_uuid = os.environ.get('BROWSERSTACK_TESTHUB_UUID', None)
        logger.debug("[collectBuildData] Collecting build data for build UUID: {}".format(build_uuid))

        try:
            endpoint = "testorchestration/api/v1/builds/{}/collect-build-data".format(build_uuid)
                        
            payload = {
                "projectName": config.get('projectName', ''),
                "buildName": config.get('buildName', os.path.basename(os.path.abspath(os.getcwd()))),
                "buildRunIdentifier": os.environ.get("BROWSERSTACK_BUILD_RUN_IDENTIFIER", ""),
                "nodeIndex": int(os.environ.get("BROWSERSTACK_NODE_INDEX") or "0"),
                "totalNodes": int(os.environ.get("BROWSERSTACK_TOTAL_NODE_COUNT") or "1"),
                "hostInfo": get_host_info(),
            }

            logger.debug("[collectBuildData] Sending build data payload: {}".format(payload))

            response = RequestUtils.post_collect_build_data(endpoint, payload)

            if response:
                logger.debug("[collectBuildData] Build data collection response: {}".format(response))
                return response
            else:
                logger.error("[collectBuildData] Failed to collect build data for build UUID: {}".format(build_uuid))
                return None

        except Exception as e:
            logger.error("[collectBuildData] Exception in collecting build data for build UUID {}: {}".format(build_uuid, e))
            return None
